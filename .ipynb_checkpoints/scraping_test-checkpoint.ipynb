{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://myanimelist.net/users.php\")\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "recentOnlineUsers = soup.find_all(class_='picSurround')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "recentSlugs = [x.find('a').get('href') for x in recentOnlineUsers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan:\n",
    "\n",
    "- For each user, find all their anime (status=7), rating for each. \n",
    "- Keep track of users, and user's watched anime in database\n",
    "- Also keep track of unique animes, average score, popularity, etc.\n",
    "\n",
    "## TODO:\n",
    "\n",
    "think of how to design database\n",
    "\n",
    "## Data to get from each user:\n",
    "\n",
    "- Anime stats\n",
    "    - \"Days\"\n",
    "    - Mean score\n",
    "    - Num watching\n",
    "    - Num completed\n",
    "    - Plan to watch\n",
    "    - List watching\n",
    "    - List completed\n",
    "    - List Plan to watch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "for profile in recentSlugs:\n",
    "    profileResp = requests.get(\"https://myanimelist.net\" + profile)\n",
    "    profileSoup = BeautifulSoup(profileResp.text, 'html.parser')\n",
    "    \n",
    "    # Completed, etc append\n",
    "    # \"?status=n\" where n = 1,2,3,4,5,6\n",
    "    \n",
    "    # class for stats looks like clearfix mb12 inside div with class stats anime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://myanimelist.net/animelist/Catalano?status=1'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile = recentSlugs[2]\n",
    "profileResp = requests.get(\"https://myanimelist.net\" + profile)\n",
    "profileSoup = BeautifulSoup(profileResp.text, 'html.parser')\n",
    "\n",
    "statsAnime = profileSoup.find(class_ = \"stats anime\")\n",
    "clearfixmb12 = statsAnime.find_all(class_=\"clearfix mb12\")\n",
    "len(clearfixmb12)\n",
    "clearfixmb12\n",
    "\n",
    "# watching, completed, on hold, dropped, plan to watch\n",
    "# hopefully, in that order.\n",
    "\n",
    "clearfixmb12[0].find('a').text # 'watching'\n",
    "clearfixmb12[0].find('span').text # '6'\n",
    "\n",
    "statScore = statsAnime.find(class_=\"stat-score\")\n",
    "statScore = [x for x in list(statScore.children) if x != '\\n'] # get two subdivs, without newlines\n",
    "statScore[0].find('span').text # 'Days: '\n",
    "statScore[0].text.split(': ') # ['Days', '213.4']\n",
    "statScore[1].text.split(': ') # ['Mena Score', '6.92']\n",
    "\n",
    "\n",
    "# list of watching:\n",
    "clearfixmb12[0].find('a').get('href')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting a list of watching, completed, etc anime is a bit trickier because the site is an infinite scrolling type. It loads at first 300 results, but you have to scroll to get more. \n",
    "\n",
    "https://myanimelist.net/animelist/Catalano?status=7\n",
    "\n",
    "This will be a case study because they've watched a lot of anime.\n",
    "\n",
    "The simple, but slow approach is to use Selenium to drive an actual browser to scroll and load the data. When it's all done, use BeautifulSoup to read everything. \n",
    "\n",
    "However, I don't want to use Selenium in this project, because I think it's a bit bulky. Since I am only scraping from one site, I figure I can do a bit of hardcoding. Theoretically, the data has to come from somewhere. \n",
    "\n",
    "https://blog.michaelyin.info/how-crawl-infinite-scrolling-pages-using-python/\n",
    "\n",
    "Is a great post on finding the request URL the site makes to the backend to retrieve the data.\n",
    "\n",
    "It looks like the response URL is https://myanimelist.net/animelist/Catalano/load.json?offset=300&status=7 for after the first 300. \n",
    "\n",
    "Going to use Python to dig deeper on how the JSON is structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 2,\n",
       " 'score': 7,\n",
       " 'tags': '',\n",
       " 'is_rewatching': 0,\n",
       " 'num_watched_episodes': 1,\n",
       " 'anime_title': 'Gatchaman Crowds Insight: Inbound',\n",
       " 'anime_num_episodes': 1,\n",
       " 'anime_airing_status': 2,\n",
       " 'anime_id': 30925,\n",
       " 'anime_studios': None,\n",
       " 'anime_licensors': None,\n",
       " 'anime_season': None,\n",
       " 'has_episode_video': False,\n",
       " 'has_promotion_video': False,\n",
       " 'has_video': False,\n",
       " 'video_url': '/anime/30925/Gatchaman_Crowds_Insight__Inbound/video',\n",
       " 'anime_url': '/anime/30925/Gatchaman_Crowds_Insight__Inbound',\n",
       " 'anime_image_path': 'https://myanimelist.cdn-dena.com/r/96x136/images/anime/2/74427.jpg?s=31f643601049374df4ee968b99f0c2d9',\n",
       " 'is_added_to_list': False,\n",
       " 'anime_media_type_string': 'ONA',\n",
       " 'anime_mpaa_rating_string': 'PG-13',\n",
       " 'start_date_string': None,\n",
       " 'finish_date_string': None,\n",
       " 'anime_start_date_string': '20-06-15',\n",
       " 'anime_end_date_string': '20-06-15',\n",
       " 'days_string': None,\n",
       " 'storage_string': '',\n",
       " 'priority_string': 'Low'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "offset0 = requests.get('https://myanimelist.net/animelist/Catalano/load.json?offset=0&status=7')\n",
    "json.loads(offset0.text)[0]\n",
    "\n",
    "offset300 = requests.get('https://myanimelist.net/animelist/Catalano/load.json?offset=300&status=7')\n",
    "json.loads(offset300.text)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`status: 2` probably refers to it being marked \"completed\". `status: 1` probably refers to it being marked \"currently watching\"\n",
    "\n",
    "It looks like if we set `offset=0` then we get the 1-300 (inclusive) anime watched. If we set `offset=300` then we get the 301-600 animes watched. What happens if the offset is more than they've watched?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offset3000 = requests.get('https://myanimelist.net/animelist/Catalano/load.json?offset=3000&status=7')\n",
    "json.loads(offset3000.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get an empty list when we try to ask for more than they've watched. I think a simple try/catch block on a list index out of range should handle the error well enough. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
